Using /home/ec2-user/.cache/torch_extensions/py37_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py37_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.6597230434417725 seconds
[2023-04-11 07:28:30,403] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown
Using /home/ec2-user/.cache/torch_extensions/py37_cu117 as PyTorch extensions root...
[2023-04-11 07:28:32,140] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-04-11 07:28:32,142] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-04-11 07:28:32,142] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-04-11 07:28:32,179] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-04-11 07:28:32,179] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-04-11 07:28:32,179] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-04-11 07:28:32,179] [INFO] [stage_1_and_2.py:144:__init__] Reduce bucket size 500000000
[2023-04-11 07:28:32,179] [INFO] [stage_1_and_2.py:145:__init__] Allgather bucket size 500000000
[2023-04-11 07:28:32,179] [INFO] [stage_1_and_2.py:146:__init__] CPU Offload: True
[2023-04-11 07:28:32,179] [INFO] [stage_1_and_2.py:147:__init__] Round robin gradient partitioning: False
Time to load utils op: 0.6028037071228027 seconds
Loading extension module utils...
Rank: 0 partition count [3] and sizes[(172779522, False)]
[2023-04-11 07:28:34,757] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-04-11 07:28:34,758] [INFO] [utils.py:834:see_memory_usage] MA 3.72 GB         Max_MA 3.72 GB         CA 3.97 GB         Max_CA 4 GB
[2023-04-11 07:28:34,759] [INFO] [utils.py:839:see_memory_usage] CPU Virtual Memory:  used = 24.84 GB, percent = 13.3%
Using /home/ec2-user/.cache/torch_extensions/py37_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[RANK 0] Collecting rollouts
[rollout 0 / 16]:   0%|                                                                               | 0/16 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ec2-user/venv/lib64/python3.7/site-packages/transformers/generation/utils.py:1202: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  "You have modified the pretrained model configuration to control generation. This is a"
[2023-04-11 07:28:36,032] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-04-11 07:28:36,033] [INFO] [utils.py:834:see_memory_usage] MA 3.72 GB         Max_MA 3.72 GB         CA 3.97 GB         Max_CA 4 GB
[2023-04-11 07:28:36,033] [INFO] [utils.py:839:see_memory_usage] CPU Virtual Memory:  used = 30.99 GB, percent = 16.6%
[2023-04-11 07:28:36,034] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-04-11 07:28:36,118] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-04-11 07:28:36,119] [INFO] [utils.py:834:see_memory_usage] MA 3.72 GB         Max_MA 3.72 GB         CA 3.97 GB         Max_CA 4 GB
[2023-04-11 07:28:36,120] [INFO] [utils.py:839:see_memory_usage] CPU Virtual Memory:  used = 30.99 GB, percent = 16.6%
[2023-04-11 07:28:36,121] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-04-11 07:28:36,121] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-04-11 07:28:36,121] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-04-11 07:28:36,121] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-06], mom=[[0.9, 0.999]]
[2023-04-11 07:28:36,123] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-04-11 07:28:36,123] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2023-04-11 07:28:36,123] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-04-11 07:28:36,123] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-04-11 07:28:36,123] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd5182bd890>
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-04-11 07:28:36,124] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 0.5}
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-04-11 07:28:36,125] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-04-11 07:28:36,126] [INFO] [config.py:1022:print]   gradient_clipping ............ 0.0
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 65536
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-04-11 07:28:36,127] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   steps_per_print .............. inf
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   train_batch_size ............. 6
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  2
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   world_size ................... 3
[2023-04-11 07:28:36,128] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  True
[2023-04-11 07:28:36,129] [INFO] [config.py:1022:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-04-11 07:28:36,129] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-04-11 07:28:36,129] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-04-11 07:28:36,129] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 2
[2023-04-11 07:28:36,129] [INFO] [config.py:1013:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2,
    "gradient_accumulation_steps": 1,
    "fp16": {
        "enabled": true,
        "min_loss_scale": 0.5,
        "fp16_scale_tolerance": 0.25,
        "opt_level": "O2",
        "auto_cast": false
    },
    "zero_optimization": {
        "stage": 2,
        "offload_param": {
            "device": "cpu"
        },
        "offload_optimizer": {
            "device": "cpu"
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 5.000000e+08,
        "contiguous_gradients": true
    },
    "steps_per_print": inf,
    "bf16": {
        "enabled": false
    },
    "zero_allow_untested_optimizer": true
}
Time to load utils op: 0.0005166530609130859 seconds
/home/ec2-user/rlhf/trlx/summarize_rlhf/trlx/trainer/accelerate_ppo_trainer.py:315: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  device=device,


[rollout 16 / 16]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  2.04s/it]
[RANK 0] Starting training
[RANK 0] Evaluating model
[generation sweep 1/1 | eval batch 1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.32s/it]
[RANK 0] Computing rewards
[3m                                              Evaluation #0 reward/mean: 0.195                                               
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m prompt                                                  [22mâ”ƒ[1m output                                                 [22mâ”ƒ[1m reward [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ SUBREDDIT: r/AskReddit                                  â”‚  I have feelings for a friend, and I can't get them    â”‚ 0.869  â”‚
â”‚ TITLE: How do you get someone out of your head?         â”‚ out of my head. How do I get them out?                 â”‚        â”‚
â”‚ POST: Hi,                                               â”‚                                                        â”‚        â”‚
â”‚ I'm 22, and I have been with my girlfriend for 5 years  â”‚                                                        â”‚        â”‚
â”‚ now. We recently moved together. We've always loved     â”‚                                                        â”‚        â”‚
â”‚ each other intensely.                                   â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ Problem, I recently started to have feelings for an     â”‚                                                        â”‚        â”‚
â”‚ other person (a friend). This person has had a          â”‚                                                        â”‚        â”‚
â”‚ boyfriend for now 3 years, and has absolutely no ideas. â”‚                                                        â”‚        â”‚
â”‚ Those feelings were so strong, it was hard to hide      â”‚                                                        â”‚        â”‚
â”‚ them. After 2 months of me being distant and really     â”‚                                                        â”‚        â”‚
â”‚ sad, my girlfriend forced me to say what was bothering  â”‚                                                        â”‚        â”‚
â”‚ me. I'm not a good liar, and now she knows.             â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ We decided to give us a week alone, I went to my        â”‚                                                        â”‚        â”‚
â”‚ parents.                                                â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ Now, I'm completely lost. I keep on thinking about this â”‚                                                        â”‚        â”‚
â”‚ person, and I hate that. I would like for those         â”‚                                                        â”‚        â”‚
â”‚ feelings to go away, to leave me alone. But I can't.    â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ What do I do? It's been 3 months now, and I'm just      â”‚                                                        â”‚        â”‚
â”‚ desperate.                                              â”‚                                                        â”‚        â”‚
â”‚ TL;DR:                                                  â”‚                                                        â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SUBREDDIT: r/pettyrevenge                               â”‚  Mom woke me up with a loud TV. I got a little revenge â”‚ 0.373  â”‚
â”‚ TITLE: So, my mom woke me up with a loud TV.            â”‚ by blasting Gangnam Style on repeat.                   â”‚        â”‚
â”‚ POST: She was in her living room, watching TV. This was â”‚                                                        â”‚        â”‚
â”‚ at about 8:30 in the morning, and she was exercising.   â”‚                                                        â”‚        â”‚
â”‚ She turned the TV up extra loud to hear it over her     â”‚                                                        â”‚        â”‚
â”‚ excercycle, and woke me up. I went in there asking for  â”‚                                                        â”‚        â”‚
â”‚ her to turn it down. She said she didn't have to; I     â”‚                                                        â”‚        â”‚
â”‚ explained that I always used headphones so she didn't   â”‚                                                        â”‚        â”‚
â”‚ have to deal with my noise and that she should give me  â”‚                                                        â”‚        â”‚
â”‚ a little more respect, given that I paid rent at the    â”‚                                                        â”‚        â”‚
â”‚ time.                                                   â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ She disagreed. I went back to my room, rather pissed    â”‚                                                        â”‚        â”‚
â”‚ off at the lack of equality. I had no lock on my door;  â”‚                                                        â”‚        â”‚
â”‚ but I had a dresser right next to it, so I pulled one   â”‚                                                        â”‚        â”‚
â”‚ of the drawers out enough so that it caused the door to â”‚                                                        â”‚        â”‚
â”‚ not be openable. Then, I turned my speakers up really   â”‚                                                        â”‚        â”‚
â”‚ loud and blasted Gangnam Style on repeat, with the bass â”‚                                                        â”‚        â”‚
â”‚ cranked up as high as it could go.                      â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ If you hate Gangnam Style for being overplayed, you     â”‚                                                        â”‚        â”‚
â”‚ will see why I chose that particular song. I personally â”‚                                                        â”‚        â”‚
â”‚ don't mind it. But here's the thing about my bass; it   â”‚                                                        â”‚        â”‚
â”‚ vibrates the walls, making one hell of a lot of noise.  â”‚                                                        â”‚        â”‚
â”‚ Needless to say, my mom was not pleased and shut off    â”‚                                                        â”‚        â”‚
â”‚ the internet. But it was oh so worth it.                â”‚                                                        â”‚        â”‚
â”‚ TL;DR:                                                  â”‚                                                        â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SUBREDDIT: r/relationships                              â”‚  Girlfriend cheated on me by kissing two guys at a     â”‚ 0.418  â”‚
â”‚ TITLE: My girlfriend (20f) of two years cheated on me   â”‚ Halloween party. I'm not sure if I should try to fix   â”‚        â”‚
â”‚ (20m) by kissing two guys at a Halloween party.         â”‚ it or just cut it off.                                 â”‚        â”‚
â”‚ POST: Lately her and I have been having a few problems, â”‚                                                        â”‚        â”‚
â”‚ and these problems have been brought up before a few    â”‚                                                        â”‚        â”‚
â”‚ times. One problem being that I don't show enough       â”‚                                                        â”‚        â”‚
â”‚ affection. I don't tell her she's pretty very often or  â”‚                                                        â”‚        â”‚
â”‚ don't compliment her much. I feel terrible about it,    â”‚                                                        â”‚        â”‚
â”‚ but this time I was really trying to change for her.    â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ For Halloween she went to visit her step brother at a   â”‚                                                        â”‚        â”‚
â”‚ college and I got drunk with my friends and watched     â”‚                                                        â”‚        â”‚
â”‚ movies. Last night (11/1) we got in a huge fight about  â”‚                                                        â”‚        â”‚
â”‚ me not changing and how our relationship won't work out â”‚                                                        â”‚        â”‚
â”‚ and basically broke up over the phone. So in an effort  â”‚                                                        â”‚        â”‚
â”‚ to try and fix it I drove to her house. She told me how â”‚                                                        â”‚        â”‚
â”‚ at the parties she went to that two guys kissed her.    â”‚                                                        â”‚        â”‚
â”‚ The first one she pushed away, but the second one I     â”‚                                                        â”‚        â”‚
â”‚ asked her if she kissed him back and she said yes and   â”‚                                                        â”‚        â”‚
â”‚ that she did it because it made her feel wanted, which  â”‚                                                        â”‚        â”‚
â”‚ I guess I haven't been making her feel that way lately. â”‚                                                        â”‚        â”‚
â”‚ We cried, we talked about everything, we had great sex, â”‚                                                        â”‚        â”‚
â”‚ and I stayed over at her house just to sleep with her   â”‚                                                        â”‚        â”‚
â”‚ and then snuck out in the morning so her parents        â”‚                                                        â”‚        â”‚
â”‚ wouldn't know.                                          â”‚                                                        â”‚        â”‚
â”‚                                                         â”‚                                                        â”‚        â”‚
â”‚ We both obviously want to work things out but aren't    â”‚                                                        â”‚        â”‚
â”‚ sure if we should. I love this girl, but the more I     â”‚                                                        â”‚        â”‚
â”‚ think about it, all I can think about is her cheating   â”‚                                                        â”‚        â”‚
â”‚ on me, and more importantly, liking it. It makes me     â”‚                                                        â”‚        â”‚
â”‚ sick to my stomach. Should I even try to fix it or      â”‚                                                        â”‚        â”‚
â”‚ would I be better off cutting all ties.                 â”‚                                                        â”‚        â”‚
â”‚ TL;DR:                                                  â”‚                                                        â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/home/ec2-user/rlhf/trlx/summarize_rlhf/trlx/trainer/accelerate_base_trainer.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dtype=float,
[RANK 0] Summarizing evaluation
[losses/total_loss: -0.00 | losses/policy_loss: -0.02 | losses/value_loss: 0.07]:   0%|      | 2/800 [00:01<12:25,  1.07it/s]
[2023-04-11 07:29:18,063] [INFO] [loss_scaler.py:180:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

[losses/total_loss: -0.00 | losses/policy_loss: -0.02 | losses/value_loss: 0.07]:   0%|      | 3/800 [00:02<12:02,  1.10it/s]










[losses/total_loss: 0.24 | losses/policy_loss: 0.21 | losses/value_loss: 0.12]:   2%|â–      | 16/800 [00:24<21:13,  1.62s/it][RANK 0] Collecting rollouts
